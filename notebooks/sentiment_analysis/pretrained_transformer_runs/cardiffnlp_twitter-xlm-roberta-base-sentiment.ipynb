{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import wandb\n",
    "import copy\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pytorch_datasets import SentimentAnalysisDataset, DatasetType\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "model_name_wandb = \"cardiffnlp_twitter-xlm-roberta-base-sentiment\" # has to be without slash\n",
    "label_arrangement = [\"Negative\", \"Neutral\", \"Positive\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Model config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load json file with hyperparams of each model\n",
    "with open('../hyperparams.json') as file:\n",
    "    hyper_params = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set up Hyper parameters for model training\n",
    "LR: float = hyper_params[model_name][\"lr\"]\n",
    "OPTIMIZER: str = hyper_params[model_name][\"optimizer\"]\n",
    "EPOCHS: int = hyper_params[model_name][\"epochs\"]\n",
    "BATCH_SIZE: int = hyper_params[model_name][\"batch_size\"]\n",
    "DROPOUT: float = hyper_params[model_name][\"dropout\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Dataframe preperations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../../data/wsb_annotations/wsb_annotations_final.xlsx\", sheet_name=\"final_annotations\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df.drop(columns=[\"stock_symbol\"], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "2    0.386\n1    0.316\n0    0.298\nName: label, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "XLMRobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Building Pytorch Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Declare generic sentiment analysis dataset without split\n",
    "sentiment_analysis_dataset = SentimentAnalysisDataset(\n",
    "    df = df,\n",
    "    tokenizer = tokenizer,\n",
    "    max_token_len = 256,\n",
    "    label_arrangement = label_arrangement\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Declare train and test dataset\n",
    "train_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TRAIN)\n",
    "test_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TEST)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Setup train and test Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=1,\n",
    "                                                drop_last=True\n",
    "                                                )\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=1,\n",
    "                                               drop_last=True\n",
    "                                              )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "800"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "200"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "TESTING DATA:\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Check if train data and test data have correct batch and tensor sizes\n",
    "\"\"\"print('TRAINING DATA:')\n",
    "for dictionary in train_data_loader:\n",
    "    print(dictionary)\n",
    "    break\"\"\"\n",
    "\n",
    "print(' ')\n",
    "print('TESTING DATA:')\n",
    "for dictionary in test_data_loader:\n",
    "    print(dictionary[\"labels\"].size())\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# pull data from test dataloader to have one batch\n",
    "\n",
    "# labels\n",
    "y_true = torch.cat(tuple(data[\"labels\"] for data in test_data_loader), dim=0).numpy().astype(int)\n",
    "\n",
    "# ids, mask, token_type_ids\n",
    "ids = torch.cat(tuple(data[\"input_ids\"] for data in test_data_loader), dim=0)\n",
    "mask = torch.cat(tuple(data[\"attention_mask\"] for data in test_data_loader), dim=0)\n",
    "token_type_ids = torch.cat(tuple(data[\"token_type_ids\"] for data in test_data_loader), dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# One last forward pass to evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    y_pred = F.one_hot(torch.argmax(outputs.logits, dim=1), num_classes=3).numpy()\n",
    "    print(y_pred)\n",
    "\n",
    "# need one more epoch before training -> epoch 0\n",
    "# need to save model to wandb or else\n",
    "# get confusion matrices right and lof them to wandb\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, recall and f1-score with micro average\n",
    "prec_avg = precision_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "recall_avg = recall_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "f1_avg = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "accuracy = (np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1)).sum() / len(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4479166666666667"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4479166666666667"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1)).sum() / len(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. WANDB Log data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjan_burger\u001B[0m (\u001B[33mhda_sis\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\janbu\\Desktop\\Bachelor_Thesis\\notebooks\\sentiment_analysis\\pretrained_transformer_runs\\wandb\\run-20220619_104225-1foqml9a</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1foqml9a\" target=\"_blank\">cardiffnlp_twitter-xlm-roberta-base-sentiment</a></strong> to <a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize WAND tracking\n",
    "config = wandb.config = {\n",
    "    \"model_name\": model_name_wandb,\n",
    "    \"type\": \"pretrained model\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=\"Bachelor-Thesis\", entity=\"hda_sis\", config=config, name=model_name_wandb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "wandb.log({\"Precision Avg\": prec_avg,\n",
    "           \"Recall Avg\": recall_avg,\n",
    "           \"F1-Score Avg\": f1_avg,\n",
    "           \"Accuracy\": accuracy\n",
    "           })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "y_true_argmax = np.argmax(y_true, axis=1)\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 2 1 0 2 0 1 2 0 2 0 0 2 1 0 0 2 2 1 0 1 1 1 2 1 0 2 1 2 2 0 1 0 1 1\n",
      " 1 0 2 0 2 1 2 0 0 1 1 0 1 1 1 2 0 1 0 2 2 0 0 2 2 1 0 2 0 2 1 2 2 2 1 2 2\n",
      " 0 2 2 0 0 2 2 2 2 1 0 2 1 2 1 2 2 1 1 2 2 1 2 0 2 1 1 1 2 2 2 2 1 0 2 2 0\n",
      " 2 2 2 0 2 2 2 2 2 1 1 2 1 1 0 0 0 0 0 0 2 0 1 1 1 2 1 2 1 2 0 1 0 1 2 1 0\n",
      " 1 0 0 1 1 0 0 1 2 0 0 0 2 0 2 1 0 0 2 2 1 1 1 1 2 2 2 0 2 0 2 1 0 0 2 0 2\n",
      " 1 0 1 2 1 2 1] [2 1 0 1 1 0 0 1 1 1 0 2 0 1 0 0 2 0 2 2 1 0 1 0 0 2 1 1 2 1 2 1 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 2 0 1 0 0 1 2 0 0 2 1 1 2 1 0 0 2 1 0 1 1 0 1 0 1 2 0 1 2 2\n",
      " 1 1 0 1 1 1 0 0 0 1 0 0 0 2 0 0 0 2 1 1 0 1 2 1 2 2 1 0 2 0 1 0 1 0 1 1 0\n",
      " 1 0 2 0 1 2 1 2 0 0 0 2 0 0 0 1 2 0 0 0 1 1 0 0 1 1 1 2 0 0 1 1 1 1 1 2 0\n",
      " 0 0 0 0 1 0 1 0 1 1 0 2 1 0 1 1 0 1 1 1 0 1 0 0 2 2 2 1 1 0 0 0 0 0 2 0 2\n",
      " 0 0 1 0 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_true_argmax, y_pred_argmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Create raw and normalized confusion matrices\n",
    "cm_raw = confusion_matrix(y_true_argmax, y_pred_argmax, labels=[0,1,2])\n",
    "cm_normalized = np.round(cm_raw.astype('float') / cm_raw.sum(axis=1)[:, np.newaxis], decimals=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[32, 20,  5],\n       [28, 28,  5],\n       [19, 29, 26]], dtype=int64)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.56, 0.35, 0.09],\n       [0.46, 0.46, 0.08],\n       [0.26, 0.39, 0.35]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_normalized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot confusion matrices micro averaged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create confusion matrices with seaborn\n",
    "\n",
    "# Raw Confusion matrix\n",
    "df_cm_raw = pd.DataFrame(cm_raw, columns=label_arrangement, index=label_arrangement)\n",
    "df_cm_raw.index.name = \"True label\"\n",
    "df_cm_raw.columns.name = \"Predicted label\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"Confusion matrix {model_name} - Raw\")\n",
    "plot_cm_raw = sns.heatmap(\n",
    "    df_cm_raw, cmap=\"Blues\", annot=True, annot_kws={\"size\": 14}\n",
    ")  # font size\n",
    "\n",
    "# Log raw confusion matrix to wandb\n",
    "wandb.log({\"Confusion matrix - Raw\": wandb.Image(plot_cm_raw)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalized Confusion matrix\n",
    "df_cm_norm = pd.DataFrame(cm_normalized, columns=label_arrangement, index=label_arrangement)\n",
    "df_cm_norm.index.name = \"True label\"\n",
    "df_cm_norm.columns.name = \"Predicted label\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"Confusion matrix {model_name} - Normalized\")\n",
    "plot_cm_norm = sns.heatmap(\n",
    "    df_cm_norm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 14}\n",
    ")  # font size\n",
    "\n",
    "# Log normalized confusion matrix to wandb\n",
    "wandb.log({\"Confusion matrix - Normalized\": wandb.Image(plot_cm_norm)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.033 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c569ac1c6314bc8914efa86d0cc394b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr><tr><td>F1-Score Avg</td><td></td></tr><tr><td>Precision Avg</td><td></td></tr><tr><td>Recall Avg</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.44792</td></tr><tr><td>F1-Score Avg</td><td>0.44792</td></tr><tr><td>Precision Avg</td><td>0.44792</td></tr><tr><td>Recall Avg</td><td>0.44792</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">cardiffnlp_twitter-xlm-roberta-base-sentiment</strong>: <a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1foqml9a\" target=\"_blank\">https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1foqml9a</a><br/>Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20220619_104225-1foqml9a\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb3877c63815427dbcfb8f8edf280741"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "XLMRobertaConfig {\n  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n  \"architectures\": [\n    \"XLMRobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"Negative\",\n    \"1\": \"Neutral\",\n    \"2\": \"Positive\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"Negative\": 0,\n    \"Neutral\": 1,\n    \"Positive\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"xlm-roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.19.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 250002\n}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch = tokenizer([\"Covid cases are increasing fast!\"], padding=True, truncation=True, max_length=256, return_tensors=\"pt\", add_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "batch = tokenizer.encode_plus(\n",
    "    \"Stocks only go up\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[     0,   1311,   5518,  50218,    621, 118055,   4271,     38,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3277,  0.1502, -0.9750]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.4741, 0.3970, 0.1289]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 256])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(ids[0], dim=0).size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([192, 256])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [53]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m----> 2\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(outputs)\n\u001B[0;32m      4\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(outputs\u001B[38;5;241m.\u001B[39mlogits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1205\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1197\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1202\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1203\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1205\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1208\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1211\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1212\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1213\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1215\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1216\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1217\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:840\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    833\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m    834\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m    835\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m    836\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m    837\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m    838\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m--> 840\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    847\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m    848\u001B[0m     embedding_output,\n\u001B[0;32m    849\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    857\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m    858\u001B[0m )\n\u001B[0;32m    859\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:133\u001B[0m, in \u001B[0;36mRobertaEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    131\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 133\u001B[0m     position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m     embeddings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m position_embeddings\n\u001B[0;32m    135\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(embeddings)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py:2183\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2177\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2178\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2179\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2180\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2181\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2182\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(ids[0:100], mask[0:100], token_type_ids[0:100])\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  6630,  1061,  9731,     4,   180,    25,  4754,  5946,    86,\n",
      "            31,   226,  6002,    30, 19302, 22717, 24236,  2124,   520,     7,\n",
      "           367,    52, 17869,   634,  2463, 33271,  2416,  1928,  7242,  2857,\n",
      "             9,  2084,  6377, 32188,  4852,    43, 42325,   783,  2603, 15402,\n",
      "           471, 15026,   256,    86,    31,    11,    94,  1470,   137,   133,\n",
      "          7204, 17869, 19302, 22717,    16,  4527,    52,    14,  5748, 23836,\n",
      "             3,  5368,  5368,   906, 16616, 21654,  1554,  1043,     3,   205,\n",
      "         48425, 34287,     9, 37008,   268, 14058,   685, 18296,     7, 39397,\n",
      "            13,  6519,   879,     7,    32,    14,   169,     6,  1329, 43964,\n",
      "            20,  9327,   527, 10994,    48, 11787, 42747, 53058,   423,    60,\n",
      "            72,    17,    18,  4548, 35530, 38370,  1250,   208,    19, 14317,\n",
      "           121, 19295,   836,   543, 17742,   612,  1364,   429,  3582,  1847,\n",
      "         50239, 50239,     3,   458,  5664, 32059,   856,     7, 37303,  9570,\n",
      "          8852,  2603,  5830,  3799,   865,    78,     8,   129,  1601,    84,\n",
      "          3805,   224,  2002,   268, 32097,   384,   109,  2505,    15,  5946,\n",
      "            16,    33,  2505,    15,  7122,     4,  1373,    14, 22979,    11,\n",
      "           263, 25392,    21,     2,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i in range(len(ids)):\n",
    "        outputs = model(torch.unsqueeze(ids[i], dim=0),\n",
    "                        torch.unsqueeze(mask[i], dim=0),\n",
    "                        torch.unsqueeze(token_type_ids[i], dim=0))\n",
    "except Exception as e:\n",
    "    print(torch.unsqueeze(ids[i], dim=0))\n",
    "    print(torch.unsqueeze(mask[i], dim=0))\n",
    "    print(torch.unsqueeze(token_type_ids[i], dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(53058)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ids[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n  \"architectures\": [\n    \"RobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"NEG\",\n    \"1\": \"NEU\",\n    \"2\": \"POS\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"NEG\": 0,\n    \"NEU\": 1,\n    \"POS\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 130,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"tokenizer_class\": \"BertweetTokenizer\",\n  \"transformers_version\": \"4.19.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}