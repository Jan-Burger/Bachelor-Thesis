{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import wandb\n",
    "import copy\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pytorch_datasets import SentimentAnalysisDataset, DatasetType\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"avichr/heBERT_sentiment_analysis\"\n",
    "model_name_wandb = \"avichr_heBERT_sentiment_analysis\" # has to be without slash\n",
    "label_arrangement = [\"Neutral\", \"Positive\", \"Negative\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Model config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load json file with hyperparams of each model\n",
    "with open('../hyperparams.json') as file:\n",
    "    hyper_params = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set up Hyper parameters for model training\n",
    "LR: float = hyper_params[model_name][\"lr\"]\n",
    "OPTIMIZER: str = hyper_params[model_name][\"optimizer\"]\n",
    "EPOCHS: int = hyper_params[model_name][\"epochs\"]\n",
    "BATCH_SIZE: int = hyper_params[model_name][\"batch_size\"]\n",
    "DROPOUT: float = hyper_params[model_name][\"dropout\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Dataframe preperations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../../data/wsb_annotations/wsb_annotations_final.xlsx\", sheet_name=\"final_annotations\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df.drop(columns=[\"stock_symbol\"], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "2    0.386\n1    0.316\n0    0.298\nName: label, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Building Pytorch Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Declare generic sentiment analysis dataset without split\n",
    "sentiment_analysis_dataset = SentimentAnalysisDataset(\n",
    "    df = df,\n",
    "    tokenizer = tokenizer,\n",
    "    max_token_len = 256,\n",
    "    label_arrangement = label_arrangement\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Declare train and test dataset\n",
    "train_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TRAIN)\n",
    "test_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TEST)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Setup train and test Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=1,\n",
    "                                                drop_last=True\n",
    "                                                )\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=1,\n",
    "                                               drop_last=True\n",
    "                                              )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "800"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "200"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "TESTING DATA:\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Check if train data and test data have correct batch and tensor sizes\n",
    "\"\"\"print('TRAINING DATA:')\n",
    "for dictionary in train_data_loader:\n",
    "    print(dictionary)\n",
    "    break\"\"\"\n",
    "\n",
    "print(' ')\n",
    "print('TESTING DATA:')\n",
    "for dictionary in test_data_loader:\n",
    "    print(dictionary[\"labels\"].size())\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# pull data from test dataloader to have one batch\n",
    "\n",
    "# labels\n",
    "y_true = torch.cat(tuple(data[\"labels\"] for data in test_data_loader), dim=0).numpy().astype(int)\n",
    "\n",
    "# ids, mask, token_type_ids\n",
    "ids = torch.cat(tuple(data[\"input_ids\"] for data in test_data_loader), dim=0)\n",
    "mask = torch.cat(tuple(data[\"attention_mask\"] for data in test_data_loader), dim=0)\n",
    "token_type_ids = torch.cat(tuple(data[\"token_type_ids\"] for data in test_data_loader), dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# One last forward pass to evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    y_pred = F.one_hot(torch.argmax(outputs.logits, dim=1), num_classes=3).numpy()\n",
    "    print(y_pred)\n",
    "\n",
    "# need one more epoch before training -> epoch 0\n",
    "# need to save model to wandb or else\n",
    "# get confusion matrices right and lof them to wandb\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, recall and f1-score with micro average\n",
    "prec_avg = precision_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "recall_avg = recall_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "f1_avg = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "accuracy = (np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1)).sum() / len(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3072916666666667"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3072916666666667"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1)).sum() / len(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. WANDB Log data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjan_burger\u001B[0m (\u001B[33mhda_sis\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\janbu\\Desktop\\Bachelor_Thesis\\notebooks\\sentiment_analysis\\pretrained_transformer_runs\\wandb\\run-20220619_095245-1kwe4qu7</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1kwe4qu7\" target=\"_blank\">avichr_heBERT_sentiment_analysis</a></strong> to <a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize WAND tracking\n",
    "config = wandb.config = {\n",
    "    \"model_name\": model_name_wandb,\n",
    "    \"type\": \"pretrained model\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=\"Bachelor-Thesis\", entity=\"hda_sis\", config=config, name=model_name_wandb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "wandb.log({\"Precision Avg\": prec_avg,\n",
    "           \"Recall Avg\": recall_avg,\n",
    "           \"F1-Score Avg\": f1_avg,\n",
    "           \"Accuracy\": accuracy\n",
    "           })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "y_true_argmax = np.argmax(y_true, axis=1)\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 1 0 2 1 2 0 1 2 1 2 2 1 0 2 2 1 1 0 2 0 0 0 1 0 2 1 0 1 1 2 0 2 0 0\n",
      " 0 2 1 2 1 0 1 2 2 0 0 2 0 0 0 1 2 0 2 1 1 2 2 1 1 0 2 1 2 1 0 1 1 1 0 1 1\n",
      " 2 1 1 2 2 1 1 1 1 0 2 1 0 1 0 1 1 0 0 1 1 0 1 2 1 0 0 0 1 1 1 1 0 2 1 1 2\n",
      " 1 1 1 2 1 1 1 1 1 0 0 1 0 0 2 2 2 2 2 2 1 2 0 0 0 1 0 1 0 1 2 0 2 0 1 0 2\n",
      " 0 2 2 0 0 2 2 0 1 2 2 2 1 2 1 0 2 2 1 1 0 0 0 0 1 1 1 2 1 2 1 0 2 2 1 2 1\n",
      " 0 2 0 1 0 1 0] [1 2 2 2 2 2 2 2 0 0 0 0 2 0 2 2 2 2 2 2 0 0 2 2 0 2 0 0 2 0 0 2 2 2 2 0 2\n",
      " 2 0 2 0 0 2 2 2 2 2 2 0 0 0 0 2 2 2 2 2 0 2 2 2 2 0 0 2 0 2 2 2 2 2 2 2 2\n",
      " 2 0 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 0 0 0 2 2 0 2 0 2 0 0 0 0 2 2 0\n",
      " 0 2 2 2 0 2 2 2 2 0 2 1 2 2 2 2 2 2 2 0 2 0 2 2 2 0 2 2 2 2 2 0 2 0 0 2 2\n",
      " 2 0 0 2 2 2 2 2 2 0 2 0 2 2 0 0 2 2 0 2 0 2 2 0 2 2 2 0 2 2 2 1 2 0 0 2 2\n",
      " 2 2 0 2 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_true_argmax, y_pred_argmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Create raw and normalized confusion matrices\n",
    "cm_raw = confusion_matrix(y_true_argmax, y_pred_argmax, labels=[0,1,2])\n",
    "cm_normalized = np.round(cm_raw.astype('float') / cm_raw.sum(axis=1)[:, np.newaxis], decimals=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[21,  2, 38],\n       [19,  1, 54],\n       [20,  0, 37]], dtype=int64)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.34, 0.03, 0.62],\n       [0.26, 0.01, 0.73],\n       [0.35, 0.  , 0.65]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_normalized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot confusion matrices micro averaged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create confusion matrices with seaborn\n",
    "\n",
    "# Raw Confusion matrix\n",
    "df_cm_raw = pd.DataFrame(cm_raw, columns=label_arrangement, index=label_arrangement)\n",
    "df_cm_raw.index.name = \"True label\"\n",
    "df_cm_raw.columns.name = \"Predicted label\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"Confusion matrix {model_name} - Raw\")\n",
    "plot_cm_raw = sns.heatmap(\n",
    "    df_cm_raw, cmap=\"Blues\", annot=True, annot_kws={\"size\": 14}\n",
    ")  # font size\n",
    "\n",
    "# Log raw confusion matrix to wandb\n",
    "wandb.log({\"Confusion matrix - Raw\": wandb.Image(plot_cm_raw)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalized Confusion matrix\n",
    "df_cm_norm = pd.DataFrame(cm_normalized, columns=label_arrangement, index=label_arrangement)\n",
    "df_cm_norm.index.name = \"True label\"\n",
    "df_cm_norm.columns.name = \"Predicted label\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"Confusion matrix {model_name} - Normalized\")\n",
    "plot_cm_norm = sns.heatmap(\n",
    "    df_cm_norm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 14}\n",
    ")  # font size\n",
    "\n",
    "# Log normalized confusion matrix to wandb\n",
    "wandb.log({\"Confusion matrix - Normalized\": wandb.Image(plot_cm_norm)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.033 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e88dd34d5d67488b9df53f0530928aa1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr><tr><td>F1-Score Avg</td><td></td></tr><tr><td>Precision Avg</td><td></td></tr><tr><td>Recall Avg</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.30729</td></tr><tr><td>F1-Score Avg</td><td>0.30729</td></tr><tr><td>Precision Avg</td><td>0.30729</td></tr><tr><td>Recall Avg</td><td>0.30729</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">avichr_heBERT_sentiment_analysis</strong>: <a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1kwe4qu7\" target=\"_blank\">https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1kwe4qu7</a><br/>Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20220619_095245-1kwe4qu7\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/677 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "666c448ad11447ddb83d74f84344d221"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/292k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97a664bb40c246859f5bc9d47c6745dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56d56cd7eebe4ac7a98a358d01d95d9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT_sentiment_analysis\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"avichr/heBERT_sentiment_analysis\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "BertConfig {\n  \"_name_or_path\": \"avichr/heBERT_sentiment_analysis\",\n  \"architectures\": [\n    \"BertForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"neutral\",\n    \"1\": \"positive\",\n    \"2\": \"negative\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"negative\": 2,\n    \"neutral\": 0,\n    \"positive\": 1\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"total_flos\": 6997313242916978688,\n  \"transformers_version\": \"4.19.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "batch = tokenizer([\"i dont like you\"], padding=True, truncation=True, max_length=256, return_tensors=\"pt\", add_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "batch = tokenizer.encode_plus(\n",
    "    \"Stocks only go up\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    0, 14659,   121,    82,    49,     2,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2467,  0.0638,  2.1333]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.0110, 0.1109, 0.8781]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 256])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(ids[0], dim=0).size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([192, 256])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [53]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m----> 2\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(outputs)\n\u001B[0;32m      4\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(outputs\u001B[38;5;241m.\u001B[39mlogits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1205\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1197\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1202\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1203\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1205\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1208\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1211\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1212\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1213\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1215\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1216\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1217\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:840\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    833\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m    834\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m    835\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m    836\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m    837\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m    838\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m--> 840\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    847\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m    848\u001B[0m     embedding_output,\n\u001B[0;32m    849\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    857\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m    858\u001B[0m )\n\u001B[0;32m    859\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:133\u001B[0m, in \u001B[0;36mRobertaEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    131\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 133\u001B[0m     position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m     embeddings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m position_embeddings\n\u001B[0;32m    135\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(embeddings)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py:2183\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2177\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2178\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2179\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2180\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2181\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2182\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(ids[0:100], mask[0:100], token_type_ids[0:100])\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  6630,  1061,  9731,     4,   180,    25,  4754,  5946,    86,\n",
      "            31,   226,  6002,    30, 19302, 22717, 24236,  2124,   520,     7,\n",
      "           367,    52, 17869,   634,  2463, 33271,  2416,  1928,  7242,  2857,\n",
      "             9,  2084,  6377, 32188,  4852,    43, 42325,   783,  2603, 15402,\n",
      "           471, 15026,   256,    86,    31,    11,    94,  1470,   137,   133,\n",
      "          7204, 17869, 19302, 22717,    16,  4527,    52,    14,  5748, 23836,\n",
      "             3,  5368,  5368,   906, 16616, 21654,  1554,  1043,     3,   205,\n",
      "         48425, 34287,     9, 37008,   268, 14058,   685, 18296,     7, 39397,\n",
      "            13,  6519,   879,     7,    32,    14,   169,     6,  1329, 43964,\n",
      "            20,  9327,   527, 10994,    48, 11787, 42747, 53058,   423,    60,\n",
      "            72,    17,    18,  4548, 35530, 38370,  1250,   208,    19, 14317,\n",
      "           121, 19295,   836,   543, 17742,   612,  1364,   429,  3582,  1847,\n",
      "         50239, 50239,     3,   458,  5664, 32059,   856,     7, 37303,  9570,\n",
      "          8852,  2603,  5830,  3799,   865,    78,     8,   129,  1601,    84,\n",
      "          3805,   224,  2002,   268, 32097,   384,   109,  2505,    15,  5946,\n",
      "            16,    33,  2505,    15,  7122,     4,  1373,    14, 22979,    11,\n",
      "           263, 25392,    21,     2,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i in range(len(ids)):\n",
    "        outputs = model(torch.unsqueeze(ids[i], dim=0),\n",
    "                        torch.unsqueeze(mask[i], dim=0),\n",
    "                        torch.unsqueeze(token_type_ids[i], dim=0))\n",
    "except Exception as e:\n",
    "    print(torch.unsqueeze(ids[i], dim=0))\n",
    "    print(torch.unsqueeze(mask[i], dim=0))\n",
    "    print(torch.unsqueeze(token_type_ids[i], dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(53058)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ids[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n  \"architectures\": [\n    \"RobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"NEG\",\n    \"1\": \"NEU\",\n    \"2\": \"POS\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"NEG\": 0,\n    \"NEU\": 1,\n    \"POS\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 130,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"tokenizer_class\": \"BertweetTokenizer\",\n  \"transformers_version\": \"4.19.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}