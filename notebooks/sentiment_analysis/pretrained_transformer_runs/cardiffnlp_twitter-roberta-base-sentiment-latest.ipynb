{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import wandb\n",
    "import copy\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pytorch_datasets import SentimentAnalysisDataset, DatasetType\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "model_name_wandb = \"cardiffnlp_twitter-roberta-base-sentiment-latest\" # has to be without slash\n",
    "label_arrangement = [\"Negative\", \"Neutral\", \"Positive\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Model config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load json file with hyperparams of each model\n",
    "with open('../hyperparams.json') as file:\n",
    "    hyper_params = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set up Hyper parameters for model training\n",
    "LR: float = hyper_params[model_name][\"lr\"]\n",
    "OPTIMIZER: str = hyper_params[model_name][\"optimizer\"]\n",
    "EPOCHS: int = hyper_params[model_name][\"epochs\"]\n",
    "BATCH_SIZE: int = hyper_params[model_name][\"batch_size\"]\n",
    "DROPOUT: float = hyper_params[model_name][\"dropout\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Dataframe preperations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../../data/wsb_annotations/wsb_annotations_final.xlsx\", sheet_name=\"final_annotations\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df.drop(columns=[\"stock_symbol\"], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "2    0.386\n1    0.316\n0    0.298\nName: label, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Building Pytorch Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Declare generic sentiment analysis dataset without split\n",
    "sentiment_analysis_dataset = SentimentAnalysisDataset(\n",
    "    df = df,\n",
    "    tokenizer = tokenizer,\n",
    "    max_token_len = 256,\n",
    "    label_arrangement = label_arrangement\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Declare train and test dataset\n",
    "train_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TRAIN)\n",
    "test_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TEST)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Setup train and test Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=1,\n",
    "                                                drop_last=True\n",
    "                                                )\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=1,\n",
    "                                               drop_last=True\n",
    "                                              )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "800"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "200"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "TESTING DATA:\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Check if train data and test data have correct batch and tensor sizes\n",
    "\"\"\"print('TRAINING DATA:')\n",
    "for dictionary in train_data_loader:\n",
    "    print(dictionary)\n",
    "    break\"\"\"\n",
    "\n",
    "print(' ')\n",
    "print('TESTING DATA:')\n",
    "for dictionary in test_data_loader:\n",
    "    print(dictionary[\"labels\"].size())\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# pull data from test dataloader to have one batch\n",
    "\n",
    "# labels\n",
    "y_true = torch.cat(tuple(data[\"labels\"] for data in test_data_loader), dim=0).numpy().astype(int)\n",
    "\n",
    "# ids, mask, token_type_ids\n",
    "ids = torch.cat(tuple(data[\"input_ids\"] for data in test_data_loader), dim=0)\n",
    "mask = torch.cat(tuple(data[\"attention_mask\"] for data in test_data_loader), dim=0)\n",
    "token_type_ids = torch.cat(tuple(data[\"token_type_ids\"] for data in test_data_loader), dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 0]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# One last forward pass to evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    y_pred = F.one_hot(torch.argmax(outputs.logits, dim=1), num_classes=3).numpy()\n",
    "    print(y_pred)\n",
    "\n",
    "# need one more epoch before training -> epoch 0\n",
    "# need to save model to wandb or else\n",
    "# get confusion matrices right and lof them to wandb\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Calculate accuracy, precision, recall and f1-score with micro average\n",
    "prec_avg = precision_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "recall_avg = recall_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "f1_avg = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "accuracy = (np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1)).sum() / len(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4947916666666667"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4947916666666667"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1)).sum() / len(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. WANDB Log data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjan_burger\u001B[0m (\u001B[33mhda_sis\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\janbu\\Desktop\\Bachelor_Thesis\\notebooks\\sentiment_analysis\\pretrained_transformer_runs\\wandb\\run-20220619_100814-1rv1qtil</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1rv1qtil\" target=\"_blank\">cardiffnlp_twitter-roberta-base-sentiment-latest</a></strong> to <a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize WAND tracking\n",
    "config = wandb.config = {\n",
    "    \"model_name\": model_name_wandb,\n",
    "    \"type\": \"pretrained model\"\n",
    "}\n",
    "\n",
    "run = wandb.init(project=\"Bachelor-Thesis\", entity=\"hda_sis\", config=config, name=model_name_wandb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "wandb.log({\"Precision Avg\": prec_avg,\n",
    "           \"Recall Avg\": recall_avg,\n",
    "           \"F1-Score Avg\": f1_avg,\n",
    "           \"Accuracy\": accuracy\n",
    "           })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "y_true_argmax = np.argmax(y_true, axis=1)\n",
    "y_pred_argmax = np.argmax(y_pred, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 2 1 0 2 0 1 2 0 2 0 0 2 1 0 0 2 2 1 0 1 1 1 2 1 0 2 1 2 2 0 1 0 1 1\n",
      " 1 0 2 0 2 1 2 0 0 1 1 0 1 1 1 2 0 1 0 2 2 0 0 2 2 1 0 2 0 2 1 2 2 2 1 2 2\n",
      " 0 2 2 0 0 2 2 2 2 1 0 2 1 2 1 2 2 1 1 2 2 1 2 0 2 1 1 1 2 2 2 2 1 0 2 2 0\n",
      " 2 2 2 0 2 2 2 2 2 1 1 2 1 1 0 0 0 0 0 0 2 0 1 1 1 2 1 2 1 2 0 1 0 1 2 1 0\n",
      " 1 0 0 1 1 0 0 1 2 0 0 0 2 0 2 1 0 0 2 2 1 1 1 1 2 2 2 0 2 0 2 1 0 0 2 0 2\n",
      " 1 0 1 2 1 2 1] [1 1 0 1 1 0 0 1 1 2 0 2 0 1 0 0 1 0 0 1 1 1 1 0 0 2 1 1 2 1 2 2 1 1 0 1 0\n",
      " 1 0 1 1 1 1 1 1 0 2 0 0 1 2 0 1 1 1 1 2 1 1 1 2 1 0 2 1 0 2 0 1 2 1 1 2 2\n",
      " 1 1 0 1 0 1 0 1 0 1 0 1 0 2 1 1 0 1 1 1 0 1 2 1 2 2 1 0 2 2 1 2 1 0 1 1 0\n",
      " 1 0 2 0 1 1 1 2 1 0 1 1 0 0 0 1 2 1 0 1 2 0 1 1 2 1 1 2 0 0 1 1 1 1 2 1 0\n",
      " 0 0 1 0 0 0 1 0 1 1 0 0 2 0 1 1 2 2 2 2 1 1 1 0 2 2 2 1 1 0 1 1 0 0 2 0 1\n",
      " 0 0 1 0 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_true_argmax, y_pred_argmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Create raw and normalized confusion matrices\n",
    "cm_raw = confusion_matrix(y_true_argmax, y_pred_argmax, labels=[0,1,2])\n",
    "cm_normalized = np.round(cm_raw.astype('float') / cm_raw.sum(axis=1)[:, np.newaxis], decimals=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[29, 24,  4],\n       [22, 35,  4],\n       [11, 32, 31]], dtype=int64)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.51, 0.42, 0.07],\n       [0.36, 0.57, 0.07],\n       [0.15, 0.43, 0.42]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_normalized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot confusion matrices micro averaged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create confusion matrices with seaborn\n",
    "\n",
    "# Raw Confusion matrix\n",
    "df_cm_raw = pd.DataFrame(cm_raw, columns=label_arrangement, index=label_arrangement)\n",
    "df_cm_raw.index.name = \"True label\"\n",
    "df_cm_raw.columns.name = \"Predicted label\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"Confusion matrix {model_name} - Raw\")\n",
    "plot_cm_raw = sns.heatmap(\n",
    "    df_cm_raw, cmap=\"Blues\", annot=True, annot_kws={\"size\": 14}\n",
    ")  # font size\n",
    "\n",
    "# Log raw confusion matrix to wandb\n",
    "wandb.log({\"Confusion matrix - Raw\": wandb.Image(plot_cm_raw)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalized Confusion matrix\n",
    "df_cm_norm = pd.DataFrame(cm_normalized, columns=label_arrangement, index=label_arrangement)\n",
    "df_cm_norm.index.name = \"True label\"\n",
    "df_cm_norm.columns.name = \"Predicted label\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"Confusion matrix {model_name} - Normalized\")\n",
    "plot_cm_norm = sns.heatmap(\n",
    "    df_cm_norm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 14}\n",
    ")  # font size\n",
    "\n",
    "# Log normalized confusion matrix to wandb\n",
    "wandb.log({\"Confusion matrix - Normalized\": wandb.Image(plot_cm_norm)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.033 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c76376f95f942d38f9469f28a5d2f4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr><tr><td>F1-Score Avg</td><td></td></tr><tr><td>Precision Avg</td><td></td></tr><tr><td>Recall Avg</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.49479</td></tr><tr><td>F1-Score Avg</td><td>0.49479</td></tr><tr><td>Precision Avg</td><td>0.49479</td></tr><tr><td>Recall Avg</td><td>0.49479</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">cardiffnlp_twitter-roberta-base-sentiment-latest</strong>: <a href=\"https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1rv1qtil\" target=\"_blank\">https://wandb.ai/hda_sis/Bachelor-Thesis/runs/1rv1qtil</a><br/>Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20220619_100814-1rv1qtil\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/929 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c0916c1f7824c17b56063f7d4a3ab2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "965af2f7181c405da6cecbad029c5746"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "208172ff462147019da28e1c13c86a98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "509417907ccb44a885795fa084491b77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96840dcfeebf43c39e9070d0f7d534b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n  \"architectures\": [\n    \"RobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"Negative\",\n    \"1\": \"Neutral\",\n    \"2\": \"Positive\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"Negative\": 0,\n    \"Neutral\": 1,\n    \"Positive\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.19.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch = tokenizer([\"Covid cases are increasing fast!\"], padding=True, truncation=True, max_length=256, return_tensors=\"pt\", add_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "batch = tokenizer.encode_plus(\n",
    "    \"Stocks only go up\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[   0,  347, 1417,  808, 1200,   32, 2284, 1769,  328,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1837,  0.0319, -1.5346]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.7236, 0.2287, 0.0477]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 256])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(ids[0], dim=0).size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([192, 256])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [53]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m----> 2\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(outputs)\n\u001B[0;32m      4\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(outputs\u001B[38;5;241m.\u001B[39mlogits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1205\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1197\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1202\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1203\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1205\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1208\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1211\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1212\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1213\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1215\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1216\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1217\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:840\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    833\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m    834\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m    835\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m    836\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m    837\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m    838\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m--> 840\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    847\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m    848\u001B[0m     embedding_output,\n\u001B[0;32m    849\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    857\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m    858\u001B[0m )\n\u001B[0;32m    859\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:133\u001B[0m, in \u001B[0;36mRobertaEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    131\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 133\u001B[0m     position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m     embeddings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m position_embeddings\n\u001B[0;32m    135\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(embeddings)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py:2183\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2177\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2178\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2179\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2180\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2181\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2182\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(ids[0:100], mask[0:100], token_type_ids[0:100])\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  6630,  1061,  9731,     4,   180,    25,  4754,  5946,    86,\n",
      "            31,   226,  6002,    30, 19302, 22717, 24236,  2124,   520,     7,\n",
      "           367,    52, 17869,   634,  2463, 33271,  2416,  1928,  7242,  2857,\n",
      "             9,  2084,  6377, 32188,  4852,    43, 42325,   783,  2603, 15402,\n",
      "           471, 15026,   256,    86,    31,    11,    94,  1470,   137,   133,\n",
      "          7204, 17869, 19302, 22717,    16,  4527,    52,    14,  5748, 23836,\n",
      "             3,  5368,  5368,   906, 16616, 21654,  1554,  1043,     3,   205,\n",
      "         48425, 34287,     9, 37008,   268, 14058,   685, 18296,     7, 39397,\n",
      "            13,  6519,   879,     7,    32,    14,   169,     6,  1329, 43964,\n",
      "            20,  9327,   527, 10994,    48, 11787, 42747, 53058,   423,    60,\n",
      "            72,    17,    18,  4548, 35530, 38370,  1250,   208,    19, 14317,\n",
      "           121, 19295,   836,   543, 17742,   612,  1364,   429,  3582,  1847,\n",
      "         50239, 50239,     3,   458,  5664, 32059,   856,     7, 37303,  9570,\n",
      "          8852,  2603,  5830,  3799,   865,    78,     8,   129,  1601,    84,\n",
      "          3805,   224,  2002,   268, 32097,   384,   109,  2505,    15,  5946,\n",
      "            16,    33,  2505,    15,  7122,     4,  1373,    14, 22979,    11,\n",
      "           263, 25392,    21,     2,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i in range(len(ids)):\n",
    "        outputs = model(torch.unsqueeze(ids[i], dim=0),\n",
    "                        torch.unsqueeze(mask[i], dim=0),\n",
    "                        torch.unsqueeze(token_type_ids[i], dim=0))\n",
    "except Exception as e:\n",
    "    print(torch.unsqueeze(ids[i], dim=0))\n",
    "    print(torch.unsqueeze(mask[i], dim=0))\n",
    "    print(torch.unsqueeze(token_type_ids[i], dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(53058)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ids[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n  \"architectures\": [\n    \"RobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"NEG\",\n    \"1\": \"NEU\",\n    \"2\": \"POS\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"NEG\": 0,\n    \"NEU\": 1,\n    \"POS\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 130,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"tokenizer_class\": \"BertweetTokenizer\",\n  \"transformers_version\": \"4.19.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 64001\n}"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}