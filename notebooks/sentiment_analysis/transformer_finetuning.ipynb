{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import yaml\n",
    "import copy\n",
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sqlalchemy import create_engine\n",
    "from pytorch_datasets import SentimentAnalysisDataset, DatasetType\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, plot_confusion_matrix, multilabel_confusion_matrix, ConfusionMatrixDisplay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"ProsusAI/finbert\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Database configuration & Model config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Reading form config.yaml\"\n",
    "with open(\"../../config.yaml\", \"r\") as yamlconfig:\n",
    "    config = yaml.load(yamlconfig, Loader=yaml.FullLoader)\n",
    "\n",
    "# Create postgres string with db-config\n",
    "postgres_username = config[\"db_config\"][\"postgres_username\"]\n",
    "postgres_password = config[\"db_config\"][\"postgres_password\"]\n",
    "postgres_address = config[\"db_config\"][\"postgres_address\"]\n",
    "postgres_port = config[\"db_config\"][\"postgres_port\"]\n",
    "postgres_dbname = config[\"db_config\"][\"postgres_dbname\"]\n",
    "\n",
    "postgres_str = f\"postgresql://{postgres_username}:{postgres_password}@{postgres_address}:{postgres_port}/{postgres_dbname}\"\n",
    "\n",
    "# create db connection with sqlalchemy\n",
    "cnx = create_engine(postgres_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load json file with hyperparams of each model\n",
    "with open('hyperparams.json') as file:\n",
    "    hyper_params = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Set up Hyper parameters for model training\n",
    "LR: float = hyper_params[model_name][\"lr\"]\n",
    "OPTIMIZER: str = hyper_params[model_name][\"optimizer\"]\n",
    "EPOCHS: int = hyper_params[model_name][\"epochs\"]\n",
    "BATCH_SIZE: int = hyper_params[model_name][\"batch_size\"]\n",
    "DROPOUT: float = hyper_params[model_name][\"dropout\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Dataframe preperations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../data/external/annotations_wsb.xlsx\", sheet_name=\"annotations_scale\")\n",
    "df.dropna(inplace=True)\n",
    "df.drop(columns=[\"ben\", \"dliden\", \"doss\", \"jmo\", \"jmo2\", \"id\"], inplace=True)\n",
    "df.rename(columns={\"Own annotation\": \"label\"}, inplace=True)\n",
    "#dummies = pd.get_dummies(df['label'])\n",
    "#df = pd.merge(\n",
    "#    left=df,\n",
    "#    right=dummies,\n",
    "#    left_index=True,\n",
    "#    right_index=True,\n",
    "#)\n",
    "#df.rename(columns={0.0: \"Negative\", 1.0: \"Neutral\", 2.0: \"Positive\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  post  label\n0    \\n  \\nGentlemen, start your boners:\\n\\nBlackBe...    2.0\n1    #It has come to my attention that $PLTR Nation...    2.0\n2    #SNDL and TLRY for smooking weed on the moon ðŸš¬...    2.0\n3                                $63k in on WKHS calls    2.0\n4                                         $CLF and $MT    1.0\n..                                                 ...    ...\n178  Woke up on the floor got up and I looked perfe...    2.0\n179  Yeah, I was going to sell my UWMC for a L, but...    0.0\n180  Yep, itâ€™s cheap right now and we could see it ...    2.0\n181  You didnâ€™t know what a hedge fund was before G...    1.0\n182                            ðŸ‘† Found the TSLA holder    2.0\n\n[183 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n  \\nGentlemen, start your boners:\\n\\nBlackBe...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#It has come to my attention that $PLTR Nation...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#SNDL and TLRY for smooking weed on the moon ðŸš¬...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$63k in on WKHS calls</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>$CLF and $MT</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>Woke up on the floor got up and I looked perfe...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>Yeah, I was going to sell my UWMC for a L, but...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>Yep, itâ€™s cheap right now and we could see it ...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>You didnâ€™t know what a hedge fund was before G...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>ðŸ‘† Found the TSLA holder</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>183 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "2.0    0.519126\n1.0    0.295082\n0.0    0.185792\nName: label, dtype: float64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts(normalize=True\n",
    "                         )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Building Pytorch Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Declare generic sentiment analysis dataset without split\n",
    "sentiment_analysis_dataset = SentimentAnalysisDataset(\n",
    "    df = df,\n",
    "    tokenizer = tokenizer,\n",
    "    max_token_len = 256\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Declare train and test dataset\n",
    "train_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TRAIN)\n",
    "test_dataset = copy.deepcopy(sentiment_analysis_dataset).set_fold(DatasetType.TEST)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Setup train and test Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=1,\n",
    "                                                drop_last=True\n",
    "                                                )\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=1,\n",
    "                                               drop_last=True\n",
    "                                              )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "146"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "37"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "TESTING DATA:\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Check if train data and test data have correct batch and tensor sizes\n",
    "\"\"\"print('TRAINING DATA:')\n",
    "for dictionary in train_data_loader:\n",
    "    print(dictionary)\n",
    "    break\"\"\"\n",
    "\n",
    "print(' ')\n",
    "print('TESTING DATA:')\n",
    "for dictionary in test_data_loader:\n",
    "    print(dictionary[\"labels\"].size())\n",
    "    #break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Creating custom Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "SentimentAnalysisModel(\n  (pretrained_model): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up custom model\n",
    "class SentimentAnalysisModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.pretrained_model(\n",
    "            input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        return output\n",
    "\n",
    "model = SentimentAnalysisModel(model)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Model training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjan_burger\u001B[0m (\u001B[33mhda_sis\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\janbu\\Desktop\\Bachelor_Thesis\\notebooks\\sentiment_analysis\\wandb\\run-20220612_210240-28yro6s9</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/hda_sis/test-project/runs/28yro6s9\" target=\"_blank\">Some random test</a></strong> to <a href=\"https://wandb.ai/hda_sis/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize WAND tracking\n",
    "wandb.config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"optimizer\": OPTIMIZER,\n",
    "    \"loss_function\": \"BCEWithLogitsLoss\"\n",
    "}\n",
    "\n",
    "wandb.init(project=\"test-project\", entity=\"hda_sis\", config=config, name=\"Some random test\")\n",
    "\n",
    "wandb.watch(models=model, log=\"all\", log_freq=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Set up Loss function and optimizer\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:49<00:00, 10.97s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    #print(epoch)\n",
    "\n",
    "    train_loss_list = list()\n",
    "    test_loss_list = list()\n",
    "\n",
    "    train_acc_list = list()\n",
    "    test_acc_list = list()\n",
    "\n",
    "    # Put model into train mode\n",
    "    model.train()\n",
    "\n",
    "    for data in train_data_loader:\n",
    "\n",
    "        # Move data to GPU\n",
    "        ids = data['input_ids'].to(device)\n",
    "        mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)\n",
    "        targets = data['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        #print(outputs)\n",
    "\n",
    "        # Check whether len of outputs is equal to len of targets\n",
    "        assert len(outputs.logits) == len(targets)\n",
    "\n",
    "        # Calculate loss loss for current batch and append it to batch_loss_list for current epoch\n",
    "        batch_loss = loss_fun(outputs.logits, targets)\n",
    "        train_loss_list.append(batch_loss.item())\n",
    "\n",
    "        # Calculate accuracy for current batch and append it to train_acc_list\n",
    "        batch_acc = ((torch.argmax(outputs.logits, dim=1) == torch.argmax(targets, dim=1)).sum()) / len(outputs.logits)\n",
    "        train_acc_list.append(batch_acc.item())\n",
    "        #print(batch_acc)\n",
    "\n",
    "        # Standard training procedure -> Empty gradients and update weights\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Delete data from GPU and empty cache\n",
    "        del ids\n",
    "        del mask\n",
    "        del token_type_ids\n",
    "        del targets\n",
    "        del outputs\n",
    "        del batch_loss\n",
    "        del batch_acc\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # Put model in evaluation mode and evaluate the model performance\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in test_data_loader:\n",
    "\n",
    "            # Move data to GPU\n",
    "            ids = data['input_ids'].to(device)\n",
    "            mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            targets = data['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            # Check whether len of outputs is equal to len of targets\n",
    "            assert len(outputs.logits) == len(targets)\n",
    "\n",
    "            # Calculate loss loss for current batch and append it to batch_loss_list for current epoch\n",
    "            test_batch_loss = loss_fun(outputs.logits, targets)\n",
    "            test_loss_list.append(test_batch_loss.item())\n",
    "\n",
    "            # Calculate accuracy for current batch and append it to train_acc_list\n",
    "            test_batch_acc = ((torch.argmax(outputs.logits, dim=1) == torch.argmax(targets, dim=1)).sum()) / len(outputs.logits)\n",
    "            test_acc_list.append(test_batch_acc.item())\n",
    "\n",
    "            # Delete data from GPU and empty cache\n",
    "            del ids\n",
    "            del mask\n",
    "            del token_type_ids\n",
    "            del targets\n",
    "            del outputs\n",
    "            del test_batch_loss\n",
    "            del test_batch_acc\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Log training process to WANDB\n",
    "    wandb.log({\"training loss\": np.mean(train_loss_list),\n",
    "               \"training accuracy\": np.mean(train_acc_list),\n",
    "               \"validation loss\": np.mean(test_loss_list),\n",
    "               \"validation accuracy\": np.mean(test_acc_list),\n",
    "               \"epoch\": epoch}, step = epoch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Model evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# pull data from test dataloader to have one batch\n",
    "\n",
    "# labels\n",
    "y_true = torch.cat(tuple(data[\"labels\"] for data in test_data_loader), dim=0).numpy().astype(int)\n",
    "\n",
    "# ids, mask, token_type_ids\n",
    "ids = torch.cat(tuple(data[\"input_ids\"] for data in test_data_loader), dim=0)\n",
    "mask = torch.cat(tuple(data[\"attention_mask\"] for data in test_data_loader), dim=0)\n",
    "token_type_ids = torch.cat(tuple(data[\"token_type_ids\"] for data in test_data_loader), dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Unwatch WANDB model to disable gradient tracking\n",
    "wandb.unwatch(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# One last forward pass to evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    y_pred = F.one_hot(torch.argmax(outputs.logits, dim=1), num_classes=3).numpy()\n",
    "    #print(y_pred)\n",
    "\n",
    "# need one more epoch before training -> epoch 0\n",
    "# need to save model to wandb or else\n",
    "# get confusion matrices right and lof them to wandb\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Final evaluation with precision, recall and F1-Score + Confusion matrix\n",
    "\n",
    "# Get scores for each class separately\n",
    "precision = precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "recall = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "f1 = f1_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "\n",
    "# Get micro-averaged scores for all classes -> see: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "\n",
    "prec_avg = precision_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "recall_avg = recall_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "f1_avg = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(precision[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# Log metrics to WANDB for current run\n",
    "# [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "wandb.log({\"Precision Positive\": precision[0],\n",
    "           \"Precision Negative\": precision[1],\n",
    "           \"Precision Neutral\": precision[2],\n",
    "           \"Recall Positive\": recall[0],\n",
    "           \"Recall Negative\": recall[1],\n",
    "           \"Recall Neutral\": recall[2],\n",
    "           \"F1-Score Positive\": f1[0],\n",
    "           \"F1-Score Negative\": f1[1],\n",
    "           \"F1-Score Neutral\": f1[2],\n",
    "           \"Precision Avg\": prec_avg,\n",
    "           \"Recall Avg\": recall_avg,\n",
    "           \"F1-Score Avg\": f1_avg\n",
    "           })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot confusion matrix\n",
    "mlcf = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(mlcf[0])\n",
    "disp.plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[16,  0],\n        [ 7,  9]],\n\n       [[22,  4],\n        [ 3,  3]],\n\n       [[13,  9],\n        [ 3,  7]]], dtype=int64)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlcf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [58]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcm\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mwandb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msklearn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot_confusion_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m})\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\wandb\\sklearn\\plot\\classifier.py:170\u001B[0m, in \u001B[0;36mconfusion_matrix\u001B[1;34m(y_true, y_pred, labels, true_labels, pred_labels, normalize)\u001B[0m\n\u001B[0;32m    167\u001B[0m correct_types \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mtest_types(y_true\u001B[38;5;241m=\u001B[39my_true, y_pred\u001B[38;5;241m=\u001B[39my_pred)\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m not_missing \u001B[38;5;129;01mand\u001B[39;00m correct_types:\n\u001B[1;32m--> 170\u001B[0m     confusion_matrix_chart \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfusion_matrix\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrue_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpred_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    179\u001B[0m     wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfusion_matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m: confusion_matrix_chart})\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\wandb\\sklearn\\calculate\\confusion_matrix.py:33\u001B[0m, in \u001B[0;36mconfusion_matrix\u001B[1;34m(y_true, y_pred, labels, true_labels, pred_labels, normalize)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconfusion_matrix\u001B[39m(\n\u001B[0;32m     21\u001B[0m     y_true\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     22\u001B[0m     y_pred\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m     normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     27\u001B[0m ):\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;124;03m\"\"\"Computes the confusion matrix to evaluate the performance of a classification.\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \n\u001B[0;32m     30\u001B[0m \u001B[38;5;124;03m    Called by plot_confusion_matrix to visualize roc curves. Please use the function\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;124;03m    plot_confusion_matrix() if you wish to visualize your confusion matrix.\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m     cm \u001B[38;5;241m=\u001B[39m \u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfusion_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     36\u001B[0m         classes \u001B[38;5;241m=\u001B[39m unique_labels(y_true, y_pred)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:309\u001B[0m, in \u001B[0;36mconfusion_matrix\u001B[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001B[0m\n\u001B[0;32m    307\u001B[0m y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m _check_targets(y_true, y_pred)\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m y_type)\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    312\u001B[0m     labels \u001B[38;5;241m=\u001B[39m unique_labels(y_true, y_pred)\n",
      "\u001B[1;31mValueError\u001B[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "wandb.log({\"cm\": wandb.sklearn.plot_confusion_matrix(y_true, y_pred)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4eee1a4acbb466492cc9bac2293e90e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1-Score Avg</td><td>â–</td></tr><tr><td>F1-Score Negative</td><td>â–</td></tr><tr><td>F1-Score Neutral</td><td>â–</td></tr><tr><td>F1-Score Positive</td><td>â–</td></tr><tr><td>Precision Avg</td><td>â–</td></tr><tr><td>Precision Negative</td><td>â–</td></tr><tr><td>Precision Neutral</td><td>â–</td></tr><tr><td>Precision Positive</td><td>â–</td></tr><tr><td>Recall Avg</td><td>â–</td></tr><tr><td>Recall Negative</td><td>â–</td></tr><tr><td>Recall Neutral</td><td>â–</td></tr><tr><td>Recall Positive</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>training accuracy</td><td>â–â–„â–…â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>training loss</td><td>â–ˆâ–…â–„â–„â–ƒâ–‚â–‚â–‚â–â–</td></tr><tr><td>validation accuracy</td><td>â–ƒâ–†â–ˆâ–†â–†â–ˆâ–ˆâ–ˆâ–â–</td></tr><tr><td>validation loss</td><td>â–ˆâ–‡â–ƒâ–„â–ƒâ–‚â–â–â–ƒâ–ƒ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1-Score Avg</td><td>0.59375</td></tr><tr><td>F1-Score Negative</td><td>0.46154</td></tr><tr><td>F1-Score Neutral</td><td>0.53846</td></tr><tr><td>F1-Score Positive</td><td>0.72</td></tr><tr><td>Precision Avg</td><td>0.59375</td></tr><tr><td>Precision Negative</td><td>0.42857</td></tr><tr><td>Precision Neutral</td><td>0.4375</td></tr><tr><td>Precision Positive</td><td>1.0</td></tr><tr><td>Recall Avg</td><td>0.59375</td></tr><tr><td>Recall Negative</td><td>0.5</td></tr><tr><td>Recall Neutral</td><td>0.7</td></tr><tr><td>Recall Positive</td><td>0.5625</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>training accuracy</td><td>0.97656</td></tr><tr><td>training loss</td><td>0.202</td></tr><tr><td>validation accuracy</td><td>0.59375</td></tr><tr><td>validation loss</td><td>0.52148</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">Some random test</strong>: <a href=\"https://wandb.ai/hda_sis/test-project/runs/28yro6s9\" target=\"_blank\">https://wandb.ai/hda_sis/test-project/runs/28yro6s9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20220612_210240-28yro6s9\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Morgen wandb einbinden in trainingsloop\n",
    "test = torch.argmax(targets, dim=1).cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 0, 2, 0, 1, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2,\n        0, 2, 2, 1, 1, 0, 2, 2], device='cuda:0')"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs.logits, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# reihenfolge der target klassen anpassen\n",
    "hello = torch.tensor([True, True, False, False]) == torch.tensor([True, True, False, False])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 10)\n",
    "len(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.5565, -1.5836,  0.8182,  0.4739, -0.0851, -0.1401, -1.4465,  0.8476,\n         -2.0555,  2.4302],\n        [-1.0015, -0.5909, -0.1389, -0.3330,  1.6120,  0.5044, -0.3765,  0.7283,\n         -0.2022,  0.7693],\n        [-1.9069, -0.1070, -0.6572, -0.2814, -0.2730, -0.6626, -0.1795,  1.0822,\n          0.2099, -1.9514],\n        [ 0.3541, -0.8508,  0.3249,  0.2214,  2.0938, -1.8457,  2.4814, -1.2087,\n          0.1469, -0.8712]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0.])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [35]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m target \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor([\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(target)\n\u001B[1;32m----> 6\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "input = torch.FloatTensor([0,1,0])\n",
    "print(input)\n",
    "target = torch.FloatTensor([1,0,0])\n",
    "print(target)\n",
    "output = loss(input, target, dim=1)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WANDB test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjan_burger\u001B[0m (\u001B[33mhda_sis\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\janbu\\Desktop\\Bachelor_Thesis\\notebooks\\sentiment_analysis\\wandb\\run-20220606_220513-chux55rd</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/hda_sis/test-project/runs/chux55rd\" target=\"_blank\">hearty-disco-1</a></strong> to <a href=\"https://wandb.ai/hda_sis/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/hda_sis/test-project/runs/chux55rd?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x17eb3323df0>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"test-project\", entity=\"hda_sis\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 128\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "wandb.log({\"loss\": 2.5})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([[0,0,1,1], [0,1,0,1], [1,0,0,1]])\n",
    "y_pred = np.array([[0,1,0,1], [0,1,0,1], [1,0,0,1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janbu\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([1. , 0.5, 0. , 1. ])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true=y_true, y_pred=y_pred, average=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "thesis",
   "language": "python",
   "display_name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}