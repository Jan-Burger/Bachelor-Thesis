{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sqlalchemy import create_engine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7816c5600ea45b4b967b7de339a3c08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae1a1d8197cb46f2b53a31ecf4a6aff9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90931cc01ed04106b5fbe0b5eb5697b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efbeafb2283243ff85f3b81c8efb091d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "res = classifier(\"I love you\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other version with specific model decleration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"I love you my dear!\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"I love you my dear!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['i', 'love', 'you', 'my', 'dear', '!']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[1045, 2293, 2017, 2026, 6203, 999]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 1045, 2293, 2017, 2026, 6203, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "batch = tokenizer([\"hello world\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.9943,  4.3083]]), hidden_states=None, attentions=None)\n",
      "tensor([[2.4782e-04, 9.9975e-01]])\n",
      "tensor([1])\n",
      "['POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)\n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# save pretrained model\n",
    "save_dir = \"testsave\"\n",
    "\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "model.save_pretrained(save_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# load saved model\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
    "\n",
    "# 1. Prepare Dataset\n",
    "\n",
    "# 2. Load a pretrained Tokenizer and call it with dataset -> encoding\n",
    "\n",
    "# 3. Build Pytorch Dataset with encodings\n",
    "\n",
    "# 4. Load pretrained Model\n",
    "\n",
    "# 5. a) load Trainer and train it -> abstracts a lot of stuff\n",
    "# 5. b) use native Pytorch training pipeline\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testload of reddit data from db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Reading form config.yaml\"\n",
    "with open(\"../../config.yaml\", \"r\") as yamlconfig:\n",
    "    config = yaml.load(yamlconfig, Loader=yaml.FullLoader)\n",
    "\n",
    "# Create postgres string with db-config\n",
    "postgres_username = config[\"db_config\"][\"postgres_username\"]\n",
    "postgres_password = config[\"db_config\"][\"postgres_password\"]\n",
    "postgres_address = config[\"db_config\"][\"postgres_address\"]\n",
    "postgres_port = config[\"db_config\"][\"postgres_port\"]\n",
    "postgres_dbname = config[\"db_config\"][\"postgres_dbname\"]\n",
    "\n",
    "postgres_str = f\"postgresql://{postgres_username}:{postgres_password}@{postgres_address}:{postgres_port}/{postgres_dbname}\"\n",
    "\n",
    "# create db connection with sqlalchemy\n",
    "cnx = create_engine(postgres_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM r_wallstreetbets_stock_symbols LIMIT 100', cnx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   id            100 non-null    int64         \n",
      " 1   post          100 non-null    object        \n",
      " 2   author        100 non-null    object        \n",
      " 3   created_at    100 non-null    datetime64[ns]\n",
      " 4   num_up_votes  100 non-null    int64         \n",
      " 5   type          100 non-null    object        \n",
      " 6   stock_symbol  100 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "     id                                               post  \\\n0     6  And here I am preaching the GME gospel at Than...   \n1     8  Oh man, I'd be selling TSLA so fast.  Everyone...   \n2    13  ?? BTX was going up all week, mooned Wednesday...   \n3    20  Every email I get is titled “Last chance!” or ...   \n4    23                      Medical is part of STEM right   \n..  ...                                                ...   \n95  919   \"Most things mentioned on this sub as a squee...   \n96  922  Thank you. “Selling covered calls makes a lot ...   \n97  923  i said that about BIDU and $262 (about it grav...   \n98  929  Been hearing PTON ads on all the podcasts I li...   \n99  938  TDOC missed earnings by 111% during a fucking ...   \n\n                  author          created_at  num_up_votes     type  \\\n0                dulkion 2021-05-01 00:47:46             2  comment   \n1            Forevergogo 2021-05-01 00:47:43             2  comment   \n2         Eattherich1987 2021-05-01 00:47:36             1  comment   \n3             JadedJared 2021-05-01 00:47:23             1  comment   \n4   SnooCauliflowers4003 2021-05-01 00:47:17             1  comment   \n..                   ...                 ...           ...      ...   \n95       Humblegiant2552 2021-05-01 03:59:25             1  comment   \n96       praetor-maximus 2021-05-01 03:59:22             1  comment   \n97              Brawldud 2021-05-01 03:59:22             1  comment   \n98          russcatalano 2021-05-01 03:59:02            -5  comment   \n99           ImAMaaanlet 2021-05-01 03:58:49             5  comment   \n\n   stock_symbol  \n0         [GME]  \n1        [TSLA]  \n2        [MVIS]  \n3          [SA]  \n4        [STEM]  \n..          ...  \n95        [GME]  \n96        [AMD]  \n97       [BIDU]  \n98       [PTON]  \n99       [TDOC]  \n\n[100 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>post</th>\n      <th>author</th>\n      <th>created_at</th>\n      <th>num_up_votes</th>\n      <th>type</th>\n      <th>stock_symbol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>And here I am preaching the GME gospel at Than...</td>\n      <td>dulkion</td>\n      <td>2021-05-01 00:47:46</td>\n      <td>2</td>\n      <td>comment</td>\n      <td>[GME]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>Oh man, I'd be selling TSLA so fast.  Everyone...</td>\n      <td>Forevergogo</td>\n      <td>2021-05-01 00:47:43</td>\n      <td>2</td>\n      <td>comment</td>\n      <td>[TSLA]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>?? BTX was going up all week, mooned Wednesday...</td>\n      <td>Eattherich1987</td>\n      <td>2021-05-01 00:47:36</td>\n      <td>1</td>\n      <td>comment</td>\n      <td>[MVIS]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>Every email I get is titled “Last chance!” or ...</td>\n      <td>JadedJared</td>\n      <td>2021-05-01 00:47:23</td>\n      <td>1</td>\n      <td>comment</td>\n      <td>[SA]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>Medical is part of STEM right</td>\n      <td>SnooCauliflowers4003</td>\n      <td>2021-05-01 00:47:17</td>\n      <td>1</td>\n      <td>comment</td>\n      <td>[STEM]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>919</td>\n      <td>\"Most things mentioned on this sub as a squee...</td>\n      <td>Humblegiant2552</td>\n      <td>2021-05-01 03:59:25</td>\n      <td>1</td>\n      <td>comment</td>\n      <td>[GME]</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>922</td>\n      <td>Thank you. “Selling covered calls makes a lot ...</td>\n      <td>praetor-maximus</td>\n      <td>2021-05-01 03:59:22</td>\n      <td>1</td>\n      <td>comment</td>\n      <td>[AMD]</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>923</td>\n      <td>i said that about BIDU and $262 (about it grav...</td>\n      <td>Brawldud</td>\n      <td>2021-05-01 03:59:22</td>\n      <td>1</td>\n      <td>comment</td>\n      <td>[BIDU]</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>929</td>\n      <td>Been hearing PTON ads on all the podcasts I li...</td>\n      <td>russcatalano</td>\n      <td>2021-05-01 03:59:02</td>\n      <td>-5</td>\n      <td>comment</td>\n      <td>[PTON]</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>938</td>\n      <td>TDOC missed earnings by 111% during a fucking ...</td>\n      <td>ImAMaaanlet</td>\n      <td>2021-05-01 03:58:49</td>\n      <td>5</td>\n      <td>comment</td>\n      <td>[TDOC]</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}